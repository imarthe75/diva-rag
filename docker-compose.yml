services:
  # Servicio de Base de Datos PostgreSQL con PgVector
  postgres_db:
    image: pgvector/pgvector:pg17
    hostname: postgres_db
    container_name: digital_vault_project-postgres-db
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      TZ: America/Mexico_City # <--- AÑADE ESTA LÍNEA
    ports:
      - "5432:5432" # Expone el puerto 5432 para acceso externo (pgAdmin, etc.)
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql # Add this line
    networks:
      - default
    healthcheck: # Opcional: Asegura que la DB esté lista antes que otros servicios
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 5s
      retries: 5
        #    command: postgres -c 'shared_preload_libraries=pg_vector'

  # Servicio de Almacenamiento de Objetos (MinIO compatible con S3)
  minio:
    image: minio/minio:latest
    hostname: minio
    container_name: digital_vault_project-minio
    ports:
      - "9000:9000" # Puerto API/Consola
      - "9001:9001" # Puerto de la consola web (si lo usas para navegar)
    environment:
      MINIO_ROOT_USER: ${CEPH_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${CEPH_SECRET_KEY}
      TZ: America/Mexico_City # <--- AÑADE ESTA LÍNEA
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    volumes:
      - minio_data:/data
    networks:
      - default

  # Servicio de Broker de Mensajes (Valkey/Redis)
  valkey:
    image: valkey/valkey:7.2.5-alpine
    hostname: valkey
    container_name: digital_vault_project-valkey
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD} # Define la contraseña de Valkey  
      TZ: America/Mexico_City # <--- AÑADE ESTA LÍNEA
    ports:
      - "6379:6379" # Expone el puerto si necesitas acceso externo
    command: valkey-server --requirepass ${REDIS_PASSWORD:-} # Usar REDIS_PASSWORD si la defines, sino sin password
    volumes:
      - valkey_data:/data
    networks:
      - default
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Servicio de Coordinación para Kafka (Zookeeper)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: digital_vault_project-zookeeper
    ports:
      - "2181:2181" # Expone el puerto si necesitas acceso externo para herramientas de Zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - default

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: digital_vault_project-kafka
    ports:
      - "9092:9092"   # Puerto para acceso desde el host (localhost:9092)
      - "29092:29092" # Puerto para comunicación interna de Docker (kafka:29092)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      
      # Define los listeners. INTERNAL para el puerto 29092, EXTERNAL para el 9092.
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      
      # *** ESTA ES LA PARTE CRÍTICA ***
      # Anuncia las direcciones para los clientes:
      # INTERNAL://kafka:29092: Para comunicación entre contenedores Docker.
      # EXTERNAL://localhost:9092: Para clientes que se conectan desde la máquina host.
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      
      # Mapeo de protocolos de seguridad a los listeners definidos.
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      
      # Este es para la comunicación entre brokers de Kafka, usa el listener interno.
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      TZ: America/Mexico_City # <--- AÑADE ESTA LÍNEA
    depends_on:
      - zookeeper
    networks:
      - default
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:29092 --list"] # Usa el puerto interno para el healthcheck
      interval: 30s
      timeout: 10s
      retries: 5

  # Servicio de Backend (Flask API)
  flask_backend:
    build:
      #context: ./backend
      context: .
      dockerfile: Dockerfile_Flask # Dockerfile para Flask en backend/Dockerfile_Flask
    hostname: flask_backend
    container_name: digital_vault_project-flask-backend
    ports:
      - "5000:5000" # Expone el puerto de la API al host
    environment:
      # Pasa todas las variables de entorno desde .env
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_HOST: postgres_db # Conexión interna al contenedor de PostgreSQL
      POSTGRES_PORT: ${POSTGRES_PORT}

      # ECELERY_BROKER_URL: redis://valkey:6379/0 # Conexión interna a Valkey
      # CELERY_RESULT_BACKEND: redis://valkey:6379/0

      # Example for flask_backend environment:
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@valkey:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@valkey:6379/0

      REDIS_PASSWORD: ${REDIS_PASSWORD}

      CEPH_ENDPOINT_URL: http://minio:9000 # Conexión interna al contenedor de MinIO
      CEPH_ACCESS_KEY: ${CEPH_ACCESS_KEY}
      CEPH_SECRET_KEY: ${CEPH_SECRET_KEY}
      CEPH_BUCKET_NAME: ${CEPH_BUCKET_NAME}

      SYSTEM_MASTER_KEY: ${SYSTEM_MASTER_KEY}
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092 # Conexión interna a Kafka

      ENABLE_KAFKA: "True"
      KAFKA_TOPIC_FILE_UPLOADED: "file_uploads" # O el nombre del topic que uses
      OLLAMA_GENERATION_MODEL: ${OLLAMA_GENERATION_MODEL} # <--- ¡CAMBIA ESTA LÍNEA! phi3:3.8b-mini-4k-instruct-q4_K_M # O mistral, o deepseek-coder

      # Si Flask necesita conectarse a Ollama, que corre en el HOST
      OLLAMA_API_BASE_URL: http://ollama:11434  # Asegúrate que esta IP sea la de tu host si no usas host.docker.internal
                                                # Para Docker Desktop en Linux, host.docker.internal funciona.
                                                # Si no, usa la IP privada del host: http://<IP_PRIVADA_HOST>:11434
      OLLAMA_EMBEDDING_MODEL: nomic-embed-text  # Ya la tienes, pero la reitero para claridad
      CLAMAV_ENABLED: "true" # Asegúrate de que ClamAV esté habilitado en el backend
      CLAMAV_HOST: "clamav" # Asegúrate de que esta variable esté definida
      CLAMAV_PORT: "3310" # Asegúrate de que esta variable esté definida
      TZ: America/Mexico_City # <--- AÑADE ESTA LÍNEA
      DOCUMENT_ENCRYPTION_KEY: ${DOCUMENT_ENCRYPTION_KEY} 
    volumes:
      - ./backend:/app # Monta el código de tu backend para que los cambios sean visibles sin reconstruir
    command: gunicorn --bind 0.0.0.0:5000 --timeout 1200 app:app # Usa Gunicorn para producción
    depends_on:
      postgres_db:
        condition: service_healthy
      valkey:
        condition: service_healthy
      minio:
        condition: service_healthy
      kafka:
        condition: service_healthy
      clamav: # <--- AÑADE ESTO para que el backend espere a ClamAV
        condition: service_healthy
          #      ollama: # Añade ollama si tu backend depende de él
          #condition: service_healthy
    networks:
      - default

    # Servicio del Worker de Celery (Optimizado)
  celery_worker:
    build:
      #context: ./backend
      context: .
      dockerfile: Dockerfile_celery # Assumes your Dockerfile for the worker is in backend/Dockerfile
    hostname: celery_worker
    container_name: digital_vault_project-celery-worker
    environment:
      # Pass all environment variables from .env
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@valkey:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@valkey:6379/0
      DOCUMENT_ENCRYPTION_KEY: ${DOCUMENT_ENCRYPTION_KEY}
      REDIS_PASSWORD: ${REDIS_PASSWORD}

      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_HOST: postgres_db # Internal connection to the PostgreSQL container
      POSTGRES_PORT: ${POSTGRES_PORT}

      CEPH_ENDPOINT_URL: http://minio:9000 # Internal connection to the MinIO container
      CEPH_ACCESS_KEY: ${CEPH_ACCESS_KEY}
      CEPH_SECRET_KEY: ${CEPH_SECRET_KEY}
      CEPH_BUCKET_NAME: ${CEPH_BUCKET_NAME}

      SYSTEM_MASTER_KEY: ${SYSTEM_MASTER_KEY}
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092 # Internal connection to Kafka

      OLLAMA_GENERATION_MODEL: ${OLLAMA_GENERATION_MODEL} # Or mistral, or deepseek-coder
      OLLAMA_API_BASE_URL: http://ollama:11434
      OLLAMA_EMBEDDING_MODEL: nomic-embed-text
      TZ: America/Mexico_City # <--- ADD THIS LINE!
    volumes:
      - ./backend:/app # Mount your backend code
    # --- OPTIMIZATION CHANGES START HERE ---
    # CORRECTED LINE: Changed --timeout to --time-limit
    command: celery -A tasks worker --loglevel=info --pool=gevent --concurrency=100 --max-tasks-per-child=50 --time-limit 600
    # Explanation of changes:
    # --pool=gevent: Switches to gevent for I/O-bound concurrency. Requires 'gevent' in requirements.txt.
    # --concurrency=100: Allows up to 100 concurrent tasks (adjust based on your server's resources and I/O patterns).
    # --max-tasks-per-child=50: Restarts worker processes after 50 tasks to prevent memory leaks.
    # --time-limit 600: Sets a hard timeout of 10 minutes (600 seconds) for tasks, killing runaway tasks.
    # --- OPTIMIZATION CHANGES END HERE ---
    depends_on:
      postgres_db:
        condition: service_healthy
      valkey:
        condition: service_healthy
      minio:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - default

  # Servicio de Monitoreo de Celery (Flower - Opcional pero Recomendado)
  flower:
    image: mher/flower:latest # Pre-built image for Celery Flower
    container_name: digital_vault_project-flower
    ports:
      - "5555:5555" # Access Flower in your browser at http://localhost:5555
    environment:
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@valkey:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@valkey:6379/0
      TZ: America/Mexico_City
    depends_on:
      - valkey # Flower needs access to the broker
      - celery_worker # Not strictly necessary for Flower to start, but good practice
    networks:
      - default

  ollama:
    image: ollama/ollama:latest
    hostname: ollama
    container_name: digital_vault_project-ollama
    environment:
      TZ: America/Mexico_City # <--- AÑADE ESTA LÍNEA
    ports:
      - "11434:11434" # Expone el puerto de Ollama al host si lo necesitas directamente, aunque los otros contenedores lo accederán por el nombre del servicio.
    volumes:
      - ollama_data:/root/.ollama # Persiste los modelos para que no los descargues cada vez
    networks:
      - default
    healthcheck: # Asegura que Ollama esté listo antes que los servicios que dependen de él
      test: ["CMD-SHELL", "curl -f http://localhost:11434 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Dale tiempo a Ollama para iniciar y cargar

  clamav:
    image: clamav/clamav-debian:1.0.9 # Utiliza una imagen oficial de ClamAV
    hostname: clamav
    container_name: digital_vault_project-clamav
    ports:
      - "3310:3310" # Exponer el puerto de clamd para acceso desde otros contenedores (y opcionalmente desde el host)
    environment:
      CLAMAV_NO_FRESHCLAM: "true" # Freshclam se ejecuta dentro del contenedor de ClamAV automáticamente
      CLAMAV_SETTINGS: | # Configuración para clamd
        Foreground yes
        TCPSocket 3310
        TCPAddr 0.0.0.0
        MaxAttempts 5
        SelfCheck 300
      - clamav_data:/var/lib/clamav
      TZ: America/Mexico_City # <--- AÑADE ESTA LÍNEA      
    volumes:
      - clamav_data:/var/lib/clamav # Persiste las definiciones de virus
    networks:
      - default
    healthcheck: # Asegura que ClamAV esté listo
      test: ["CMD-SHELL", "clamdscan --ping || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 120s # Dale tiempo para que freshclam actualice las definiciones

# ... (el resto de tus servicios)

# Definición de Redes
networks:
  default:
    driver: bridge

# Definición de Volúmenes (para persistencia de datos)
volumes:
  postgres_data:
  valkey_data:
  minio_data:
  ollama_data: # <--- ¡AÑADE ESTA LÍNEA!
  clamav_data: # Define el volumen persistente para ClamAV
